{"title":"System Architecture","markdown":{"yaml":{"title":"System Architecture","description":"A technical overview of RED-X system components, data sources, and workflows."},"headingText":"1.0 Overview","containsRefs":false,"markdown":"\n\n\n\nThe system architecture section provides a high-level overview of how RED-X is built and how its components interact to deliver a seamless user experience. It outlines the core technologies used, the data flow from external sources to the app interface, and how the system stays up-to-date through automated processes.\n\nRED-X is a browser-based web application developed using **R Shiny** and hosted on **shinyapps.io**. It connects to a **local relational database** (SQLite) that is updated every 48 hours via an API integration with the **Borealis Dataverse**. The system is optimized for metadata visualization, interactive filtering, and reusability analysis.\n\nThis section is intended for developers, technical reviewers, and advanced users who want to understand how RED-X integrates data from the Borealis Dataverse, processes it for analysis, and presents it through an interactive Shiny interface. By the end of this section, you'll have a clear picture of how RED-X integrates with external data sources, organizes content in a local database, and serves dynamic outputs through its Shiny-based front end.\n\n\n\n### 2.0 Key components\n\nRED-X is composed of several key components that work together to deliver a smooth and responsive data exploration experience. Each part plays a specific role in fetching, preparing, and displaying data to users.\n\n| Component | Description |\n|----------|-------------|\n| **Frontend (User Interface)** | Built with **R Shiny**, the frontend is modular and reactive. It includes:<br>- Carousel-style home summary<br>- Keyword/author network visualization<br>- Data Explorer with filtering<br>- Tooltip and sidebar interactivity<br>- Dynamic filters, value boxes, tables, and visualizations<br>- Hosted on [shinyapps.io](https://www.shinyapps.io) |\n| **Backend (Data Engine)** | - Uses a lightweight **SQLite database** stored locally<br>- Stores cleaned and structured metadata from the Borealis Dataverse<br>- Includes tables for metadata, files, keywords, authors, and network edges<br>- Optimized for fast queries and minimal setup |\n| **API Integration** | - Connects to the **Borealis Dataverse API**<br>- Retrieves metadata, file listings, DOIs, and keyword-author tags<br>- Data is fetched as JSON and processed into tidy tabular format |\n| **Scheduled Update Process** | - Runs every **48 hours** to refresh the database<br>- Uses an automated R script to:<br>&nbsp;&nbsp;&nbsp;• Pull updated metadata and files<br>&nbsp;&nbsp;&nbsp;• Clean and transform the data<br>&nbsp;&nbsp;&nbsp;• Merge into the SQLite database<br>- Keeps the app in sync with Borealis |\n\n> **Note:** Some of these key features are explained in more detail in the sections that follow.\n\n\n\n### 3.0 Data Flow Pipeline\n\nThe data flow in RED-X follows a streamlined, automated process that ensures users always have access to the most up-to-date study metadata and data files.\n\nData originates from the [**Borealis Dataverse**](https://borealisdata.ca/dataverse/ugardr), where it is retrieved using API calls. Once fetched, the data is cleaned, transformed, and stored in a local SQLite database, which the app uses to deliver fast and filtered responses to users in real time.The diagram below summarizes the process. Each component plays a specific role in moving data through the system.\n\n![](images/data_flow.png){width=85%}\n\n#### 3.1 Fetching Data from Borealis using API calls\n\n##### 1. API Integration for data fetching\n\nRED-X connects to the [Borealis Dataverse API](https://guides.dataverse.org/en/latest/api/) to fetch metadata, datasets, and file information from public and restricted research repositories. This guide provides a high-level overview of how the API is integrated into the app.\n\nThe RED-X app integrates with the Borealis Dataverse API to automatically retrieve and update metadata, authorship information, keywords, DOIs, and associated research files. This integration allows RED-X to stay synchronized with the most recent dataset updates on Borealis, without requiring manual downloads or uploads. It streamlines the process of discovering and exploring historical research data.\n\nAPI calls are made using custom R functions that fetch and process the data into a clean, structured format. Access to the API is secured using a user-specific token stored in environment variables, ensuring both security and flexibility. Depending on user permissions (e.g., general user vs. superuser), access levels to certain data may vary.\n\n\n> **Note:** Detailed information about the API integrations and functions can be found at the [API Guide](api.qmd) section.\n\n\n##### 2. **Data cleaning, transformation**\n\nAfter data is fetched from the [**Borealis Dataverse**](https://borealisdata.ca/dataverse/ugardr) via API calls, it undergoes a structured pipeline that cleans, transforms, and stores the information in a **lightweight SQLite database** for fast retrieval and application use.\n\nThe raw metadata and file listings are parsed and cleaned using a combination of R packages like `dplyr`, `tidyr`, and `stringr`. Below are the main steps:\n\n| **Step**                     | **Description**                                                                                                                                      |\n|-----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Flatten JSON Responses**  | Nested JSON responses from the API are converted into tabular format using `jsonlite::fromJSON(..., flatten = TRUE)`.                              |\n| **Filter Valid Datasets**   | Entries are filtered to ensure they have a valid `persistent_id` (DOI) and required metadata.                                                       |\n| **Extract and Normalize**   | Metadata such as authors, keywords, temporal coverage, and spatial coverage are parsed and stored in a tidy format.                                 |\n| **String Cleanup**          | Fields like keywords and authors are cleaned to remove extraneous punctuation, whitespace, or formatting artifacts.                                 |\n| **Deduplication**           | Identical entries are removed to avoid redundancy using `dplyr::distinct()`.                                                                       |\n| **File Filtering**          | Only `.tab`, `.csv`, and metadata `.txt` files are retained. A helper function `filter_filelist()` ensures correct file extensions are selected.    |\n\n```r\n# Example: Clean a raw keyword string\ncleaned_keywords <- str_split(raw_keywords, \";\\\\s*\") %>%\n  unlist() %>%\n  str_replace_all(\"[^a-zA-Z0-9\\\\s-]\", \"\") %>%\n  str_squish() %>%\n  str_to_title() %>%\n  unique()\n```\n\n##### 3. **Storage in SQLite Database**\n\nOnce cleaned, the data is stored in an SQLite database bundled with the Shiny app. This makes querying and updating lightweight, portable, and fast.\n\n| **Table**              | **Purpose**                                                                                                      |\n|------------------------|------------------------------------------------------------------------------------------------------------------|\n| `research_data`        | Stores the core metadata of all studies (DOI, title, authors, publication date, etc.)                           |\n| `update_info`          | Keeps a timestamp of the last update to prevent redundant API calls (refresh every 48 hours)                    |\n| `keywords_node/edge`   | Stores the network data for keyword co-occurrence                                                               |\n| `authors_node/edge`    | Stores the network data for author collaboration                                                                |\n| `college_colors`       | Color mapping for each college used in the network visualization                                                |\n| `department_colors`    | Color mapping for each department used in the network visualization                                             |\n\n```r\n# Example: Save cleaned study metadata to SQLite\ndbWriteTable(conn, \"research_data\", cleaned_metadata, append = TRUE)\n```\n\n> The SQLite database is accessed by the Shiny app at runtime to populate the user interface with up-to-date and searchable content.\n\n##### 4. **Scheduled Updates**\n\nA background R script checks if an update is needed (based on a 48-hour interval) and refreshes the database only when new datasets are detected. This ensures a responsive app while minimizing API load.\n\n```r\n # Check if update_info table exists and when it was last updated\n  if (\"update_info\" %in% dbListTables(conn)) {\n    update_info <- dbReadTable(conn, \"update_info\")\n    if (nrow(update_info) > 0) {\n      last_update <- as.POSIXct(update_info$last_update[1])\n      time_diff <- difftime(Sys.time(), last_update, units = \"hours\")\n      if (time_diff < 48) {\n        update_needed <- FALSE\n        message(\"Less than 48 hours since last update (\", round(time_diff, 2), \" hours). Using cached data.\")\n      }\n    }\n  }\n  \n  if (!update_needed && (\"research_data\" %in% dbListTables(conn))) {\n    # Return cached data if no update is needed\n    return(dbReadTable(conn, \"research_data\"))\n  }\n  \n  message(\"Updating cache with new data...\")\n```\n\n> This automated pipeline ensures that users always see the most recent research metadata available in the Borealis repository.\n\n### 4.0 Deployment\n\nThe RED‑X application is currently deployed using shinyapps.io, a cloud-based hosting service for Shiny applications by RStudio (Posit). This enables the app to be publicly accessible from any browser without requiring local installation of R or its dependencies. \n\nThe **RED-X App** is publicly accessible via the following link: [Launch RED-X on shinyapps.io](https://agrifooddatacanada.shinyapps.io/RED-X/)\n\nThis app is actively version-controlled and maintained through GitHub. You can view the full source code, contribute, or report issues using the repository link below: [View RED-X on GitHub](https://github.com/agrifooddatacanada/OAC_Historical_Research_Data_Explorer_App)\n\nThe GitHub repository contains:\n- All the source code (UI and server components)\n- Scripts for API integration and data processing\n- Deployment and update scripts\n- Project documentation and development history\n\n\n> Tip: For details on local development or contributing, see the [Developer Guide](dev.qmd) section.","srcMarkdownNoYaml":"\n\n\n### 1.0 Overview\n\nThe system architecture section provides a high-level overview of how RED-X is built and how its components interact to deliver a seamless user experience. It outlines the core technologies used, the data flow from external sources to the app interface, and how the system stays up-to-date through automated processes.\n\nRED-X is a browser-based web application developed using **R Shiny** and hosted on **shinyapps.io**. It connects to a **local relational database** (SQLite) that is updated every 48 hours via an API integration with the **Borealis Dataverse**. The system is optimized for metadata visualization, interactive filtering, and reusability analysis.\n\nThis section is intended for developers, technical reviewers, and advanced users who want to understand how RED-X integrates data from the Borealis Dataverse, processes it for analysis, and presents it through an interactive Shiny interface. By the end of this section, you'll have a clear picture of how RED-X integrates with external data sources, organizes content in a local database, and serves dynamic outputs through its Shiny-based front end.\n\n\n\n### 2.0 Key components\n\nRED-X is composed of several key components that work together to deliver a smooth and responsive data exploration experience. Each part plays a specific role in fetching, preparing, and displaying data to users.\n\n| Component | Description |\n|----------|-------------|\n| **Frontend (User Interface)** | Built with **R Shiny**, the frontend is modular and reactive. It includes:<br>- Carousel-style home summary<br>- Keyword/author network visualization<br>- Data Explorer with filtering<br>- Tooltip and sidebar interactivity<br>- Dynamic filters, value boxes, tables, and visualizations<br>- Hosted on [shinyapps.io](https://www.shinyapps.io) |\n| **Backend (Data Engine)** | - Uses a lightweight **SQLite database** stored locally<br>- Stores cleaned and structured metadata from the Borealis Dataverse<br>- Includes tables for metadata, files, keywords, authors, and network edges<br>- Optimized for fast queries and minimal setup |\n| **API Integration** | - Connects to the **Borealis Dataverse API**<br>- Retrieves metadata, file listings, DOIs, and keyword-author tags<br>- Data is fetched as JSON and processed into tidy tabular format |\n| **Scheduled Update Process** | - Runs every **48 hours** to refresh the database<br>- Uses an automated R script to:<br>&nbsp;&nbsp;&nbsp;• Pull updated metadata and files<br>&nbsp;&nbsp;&nbsp;• Clean and transform the data<br>&nbsp;&nbsp;&nbsp;• Merge into the SQLite database<br>- Keeps the app in sync with Borealis |\n\n> **Note:** Some of these key features are explained in more detail in the sections that follow.\n\n\n\n### 3.0 Data Flow Pipeline\n\nThe data flow in RED-X follows a streamlined, automated process that ensures users always have access to the most up-to-date study metadata and data files.\n\nData originates from the [**Borealis Dataverse**](https://borealisdata.ca/dataverse/ugardr), where it is retrieved using API calls. Once fetched, the data is cleaned, transformed, and stored in a local SQLite database, which the app uses to deliver fast and filtered responses to users in real time.The diagram below summarizes the process. Each component plays a specific role in moving data through the system.\n\n![](images/data_flow.png){width=85%}\n\n#### 3.1 Fetching Data from Borealis using API calls\n\n##### 1. API Integration for data fetching\n\nRED-X connects to the [Borealis Dataverse API](https://guides.dataverse.org/en/latest/api/) to fetch metadata, datasets, and file information from public and restricted research repositories. This guide provides a high-level overview of how the API is integrated into the app.\n\nThe RED-X app integrates with the Borealis Dataverse API to automatically retrieve and update metadata, authorship information, keywords, DOIs, and associated research files. This integration allows RED-X to stay synchronized with the most recent dataset updates on Borealis, without requiring manual downloads or uploads. It streamlines the process of discovering and exploring historical research data.\n\nAPI calls are made using custom R functions that fetch and process the data into a clean, structured format. Access to the API is secured using a user-specific token stored in environment variables, ensuring both security and flexibility. Depending on user permissions (e.g., general user vs. superuser), access levels to certain data may vary.\n\n\n> **Note:** Detailed information about the API integrations and functions can be found at the [API Guide](api.qmd) section.\n\n\n##### 2. **Data cleaning, transformation**\n\nAfter data is fetched from the [**Borealis Dataverse**](https://borealisdata.ca/dataverse/ugardr) via API calls, it undergoes a structured pipeline that cleans, transforms, and stores the information in a **lightweight SQLite database** for fast retrieval and application use.\n\nThe raw metadata and file listings are parsed and cleaned using a combination of R packages like `dplyr`, `tidyr`, and `stringr`. Below are the main steps:\n\n| **Step**                     | **Description**                                                                                                                                      |\n|-----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Flatten JSON Responses**  | Nested JSON responses from the API are converted into tabular format using `jsonlite::fromJSON(..., flatten = TRUE)`.                              |\n| **Filter Valid Datasets**   | Entries are filtered to ensure they have a valid `persistent_id` (DOI) and required metadata.                                                       |\n| **Extract and Normalize**   | Metadata such as authors, keywords, temporal coverage, and spatial coverage are parsed and stored in a tidy format.                                 |\n| **String Cleanup**          | Fields like keywords and authors are cleaned to remove extraneous punctuation, whitespace, or formatting artifacts.                                 |\n| **Deduplication**           | Identical entries are removed to avoid redundancy using `dplyr::distinct()`.                                                                       |\n| **File Filtering**          | Only `.tab`, `.csv`, and metadata `.txt` files are retained. A helper function `filter_filelist()` ensures correct file extensions are selected.    |\n\n```r\n# Example: Clean a raw keyword string\ncleaned_keywords <- str_split(raw_keywords, \";\\\\s*\") %>%\n  unlist() %>%\n  str_replace_all(\"[^a-zA-Z0-9\\\\s-]\", \"\") %>%\n  str_squish() %>%\n  str_to_title() %>%\n  unique()\n```\n\n##### 3. **Storage in SQLite Database**\n\nOnce cleaned, the data is stored in an SQLite database bundled with the Shiny app. This makes querying and updating lightweight, portable, and fast.\n\n| **Table**              | **Purpose**                                                                                                      |\n|------------------------|------------------------------------------------------------------------------------------------------------------|\n| `research_data`        | Stores the core metadata of all studies (DOI, title, authors, publication date, etc.)                           |\n| `update_info`          | Keeps a timestamp of the last update to prevent redundant API calls (refresh every 48 hours)                    |\n| `keywords_node/edge`   | Stores the network data for keyword co-occurrence                                                               |\n| `authors_node/edge`    | Stores the network data for author collaboration                                                                |\n| `college_colors`       | Color mapping for each college used in the network visualization                                                |\n| `department_colors`    | Color mapping for each department used in the network visualization                                             |\n\n```r\n# Example: Save cleaned study metadata to SQLite\ndbWriteTable(conn, \"research_data\", cleaned_metadata, append = TRUE)\n```\n\n> The SQLite database is accessed by the Shiny app at runtime to populate the user interface with up-to-date and searchable content.\n\n##### 4. **Scheduled Updates**\n\nA background R script checks if an update is needed (based on a 48-hour interval) and refreshes the database only when new datasets are detected. This ensures a responsive app while minimizing API load.\n\n```r\n # Check if update_info table exists and when it was last updated\n  if (\"update_info\" %in% dbListTables(conn)) {\n    update_info <- dbReadTable(conn, \"update_info\")\n    if (nrow(update_info) > 0) {\n      last_update <- as.POSIXct(update_info$last_update[1])\n      time_diff <- difftime(Sys.time(), last_update, units = \"hours\")\n      if (time_diff < 48) {\n        update_needed <- FALSE\n        message(\"Less than 48 hours since last update (\", round(time_diff, 2), \" hours). Using cached data.\")\n      }\n    }\n  }\n  \n  if (!update_needed && (\"research_data\" %in% dbListTables(conn))) {\n    # Return cached data if no update is needed\n    return(dbReadTable(conn, \"research_data\"))\n  }\n  \n  message(\"Updating cache with new data...\")\n```\n\n> This automated pipeline ensures that users always see the most recent research metadata available in the Borealis repository.\n\n### 4.0 Deployment\n\nThe RED‑X application is currently deployed using shinyapps.io, a cloud-based hosting service for Shiny applications by RStudio (Posit). This enables the app to be publicly accessible from any browser without requiring local installation of R or its dependencies. \n\nThe **RED-X App** is publicly accessible via the following link: [Launch RED-X on shinyapps.io](https://agrifooddatacanada.shinyapps.io/RED-X/)\n\nThis app is actively version-controlled and maintained through GitHub. You can view the full source code, contribute, or report issues using the repository link below: [View RED-X on GitHub](https://github.com/agrifooddatacanada/OAC_Historical_Research_Data_Explorer_App)\n\nThe GitHub repository contains:\n- All the source code (UI and server components)\n- Scripts for API integration and data processing\n- Deployment and update scripts\n- Project documentation and development history\n\n\n> Tip: For details on local development or contributing, see the [Developer Guide](dev.qmd) section."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-after-body":["zoom.js"],"number-sections":false,"css":["styles.css"],"toc":true,"toc-depth":5,"output-file":"architecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":"flatly","dark":"darkly"},"toc-title":"On this page","title":"System Architecture","description":"A technical overview of RED-X system components, data sources, and workflows."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}